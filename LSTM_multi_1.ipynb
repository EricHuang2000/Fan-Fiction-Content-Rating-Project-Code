{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9bac1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_30_sentences = pd.read_csv(r'C:\\Users\\erich\\Desktop\\DS_project\\data\\data_30_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488a2788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting daal==2021.4.0\n",
      "  Downloading daal-2021.4.0-py2.py3-none-win_amd64.whl (69.0 MB)\n",
      "Collecting tbb==2021.*\n",
      "  Downloading tbb-2021.10.0-py3-none-win_amd64.whl (284 kB)\n",
      "Installing collected packages: tbb, daal\n",
      "  Attempting uninstall: tbb\n",
      "    Found existing installation: TBB 0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    }
   ],
   "source": [
    "pip install daal==2021.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c26564",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.21.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13419741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.25.1\n",
      "Uninstalling numpy-1.25.1:\n",
      "  Successfully uninstalled numpy-1.25.1\n",
      "Found existing installation: tensorflow 2.13.0\n",
      "Uninstalling tensorflow-2.13.0:\n",
      "  Successfully uninstalled tensorflow-2.13.0\n",
      "Requirement already satisfied: numpy in c:\\users\\erich\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.56.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\erich\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (3.0.4)\n",
      "Installing collected packages: numpy, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "Successfully installed numpy-1.24.3 tensorflow-2.13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy tensorflow\n",
    "!pip install numpy tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a615ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               HTML_Content                                        TXT_Content\n",
      "0     Teen And Up Audiences  huckleberry finn stood front old wooden door m...\n",
      "1     Teen And Up Audiences  right huck trying figure every sort way could ...\n",
      "2         General Audiences  three buried mother christmas eve thing huck c...\n",
      "3     Teen And Up Audiences  air hot brow wet mind exhausted st petersburg ...\n",
      "4         General Audiences  episode genre al chaos chaos space time vortex...\n",
      "...                     ...                                                ...\n",
      "5993  Teen And Up Audiences  seen news pat asked robin chance hang coat mor...\n",
      "5994  Teen And Up Audiences  free next friday night murphy smiled phone rob...\n",
      "5995                 Mature  turned walked away wan na say come baby give w...\n",
      "5996      General Audiences  perfect word tended apply liberally one often ...\n",
      "5997                 Mature  heaven fact unlike earth pro con pro include t...\n",
      "\n",
      "[5998 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df[df['HTML_Content'].isin(['Explicit', 'General Audiences', 'Mature', 'Not Rated', 'Teen And Up Audiences'])]\n",
    "\n",
    "    df = df.dropna(subset=['TXT_Content'])\n",
    "\n",
    "    def clean_text(text):\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = text.lower()\n",
    "        tokenized_text = word_tokenize(text)\n",
    "        cleaned_text = [lemmatizer.lemmatize(word) for word in tokenized_text if word not in set(stopwords.words('english'))]\n",
    "\n",
    "        return ' '.join(cleaned_text)\n",
    "\n",
    "    df['TXT_Content'] = df['TXT_Content'].apply(clean_text)\n",
    "\n",
    "    return df\n",
    "\n",
    "data_30_sentences = clean_data(data_30_sentences)\n",
    "\n",
    "print(data_30_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fac99e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erich\\AppData\\Local\\Temp\\ipykernel_28200\\475069023.py:34: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, TimeDistributed, Flatten, Attention\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.callbacks import EarlyStopping\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# Pre-processing\n",
    "max_length = 300\n",
    "embedding_dim = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_30_sentences['TXT_Content'])\n",
    "sequences = tokenizer.texts_to_sequences(data_30_sentences['TXT_Content'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "labels = to_categorical(np.asarray(LabelEncoder().fit_transform(data_30_sentences['HTML_Content'])))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Word Embedding\n",
    "glove_input_file = 'C:\\\\Users\\\\erich\\\\Desktop\\\\DS_project\\\\code\\\\glove.6B\\\\glove.6B.100d.txt'\n",
    "word2vec_output_file = 'C:\\\\Users\\\\erich\\\\Desktop\\\\DS_project\\\\code\\\\glove.6B\\\\glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a50c549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a sequence: [3822, 3154, 255, 141, 87, 1367, 52, 201, 4911, 1155, 3253, 4658, 210, 450, 2810, 6, 322, 18, 66, 251, 176, 497, 5432, 366, 5, 1215, 4659, 895, 10855, 268, 296, 2901, 16, 81, 461, 111, 156, 1768, 296, 72, 2666, 134, 30, 756, 4, 314, 16, 371, 720, 41, 3878, 6558, 27192, 273, 3536, 10278, 81, 137, 680, 7943, 1135, 3536, 3376, 985, 6385, 11471, 714, 279, 25, 48, 5920, 499, 1225, 10855, 116, 16963, 856, 281, 79, 3878, 10279, 1070, 322, 18, 1870, 84, 305, 205, 1628, 4425, 52, 357, 33, 564, 22, 6, 3536, 8, 1785, 14051, 3289, 4053, 771, 1753, 714, 3496, 3673, 714, 7944, 3253, 90, 5433, 383, 71, 579, 8, 168, 137, 579, 322, 18, 207, 4, 553, 1, 17, 318, 4, 553, 1, 17, 318, 236, 25, 296, 55, 579, 765, 163, 439, 669, 560, 30, 3377, 2835, 55, 276, 49, 253, 93, 378, 4290, 13, 2066, 3, 10, 182, 264, 79, 119, 2106, 2489, 19, 33, 3822, 34, 4660, 49, 79, 40, 378, 13, 2066, 76, 34, 656, 724, 21985, 2066, 49, 714, 650, 7, 1455, 15, 10856, 118, 774, 246, 6, 24, 78, 79, 280, 178, 12, 730, 8, 106, 3338, 79, 280, 155, 6559]\n",
      "Example of a label: [0. 0. 0. 0. 1.]\n",
      "Embedding matrix: [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.039741    0.035052    0.32988    ... -0.26137     0.55432999\n",
      "   0.24510001]\n",
      " [-0.22556999  0.49417999  0.48609999 ... -0.45743999  0.49645999\n",
      "   0.34906   ]\n",
      " ...\n",
      " [ 0.26673999  0.38225001  0.47315001 ...  0.12543    -0.48914\n",
      "   0.60215998]\n",
      " [-0.049455    0.31413999  0.23686001 ... -0.23169    -0.48556\n",
      "   0.82384002]\n",
      " [-0.41146001 -0.25839001  0.33755001 ... -0.28806999  0.33107001\n",
      "   0.18941   ]]\n",
      "Word: would, Vector: [ 3.97409983e-02  3.50520015e-02  3.29879999e-01 -5.87899983e-01\n",
      " -2.50539988e-01 -4.07189995e-01 -1.39039993e-01  4.73930001e-01\n",
      "  5.99050000e-02  8.99960026e-02 -2.15210006e-01  5.65330029e-01\n",
      "  3.00989985e-01 -2.86579996e-01 -5.57300001e-02 -7.87240028e-01\n",
      "  4.86559987e-01  5.83209991e-01 -8.61989975e-01  2.09240004e-01\n",
      "  4.67400014e-01 -3.63959998e-01  3.05260003e-01  4.21850011e-02\n",
      " -5.36019988e-02 -2.72020012e-01 -2.85849988e-01 -5.41700006e-01\n",
      "  2.96779990e-01 -4.08679992e-01  9.73810032e-02  8.10010016e-01\n",
      " -2.08639994e-01  1.84780002e-01  1.87570006e-01  8.55799973e-01\n",
      "  4.40710008e-01  2.52180010e-01 -1.81480005e-01 -1.91560000e-01\n",
      " -5.83450019e-01 -2.25810006e-01  1.73040002e-03 -5.08610010e-01\n",
      " -2.36159995e-01 -1.68890003e-02  4.24600005e-01 -5.05659997e-01\n",
      " -1.47100002e-01 -1.23389995e+00  1.25780003e-02 -1.78979993e-01\n",
      "  1.28619999e-01  1.28980005e+00 -1.45190001e-01 -2.55570006e+00\n",
      " -6.91279992e-02 -4.79169995e-01  1.83589995e+00  1.03450000e+00\n",
      " -1.36040002e-01  5.82360029e-01 -4.88739997e-01  1.18409999e-01\n",
      "  1.18589997e+00  1.00330003e-01  2.63200015e-01  1.00950003e+00\n",
      " -3.47849995e-01 -9.30800021e-01  4.15170014e-01 -1.17739999e+00\n",
      " -3.34769994e-01 -5.25059998e-01  4.77759987e-02 -2.33899996e-01\n",
      " -2.28569999e-01  3.10050011e-01 -1.06550002e+00  5.11510000e-02\n",
      "  7.21589983e-01 -5.25560021e-01 -4.58819985e-01 -1.67290002e-01\n",
      " -1.88689995e+00 -4.89960015e-01  8.22109997e-01  7.79169984e-03\n",
      "  7.23140016e-02 -4.67949986e-01 -1.33589998e-01 -3.27969998e-01\n",
      " -2.75050014e-01 -3.13419998e-01 -6.77630007e-01  1.59729999e-02\n",
      "  2.84869999e-01 -2.61370003e-01  5.54329991e-01  2.45100006e-01]\n",
      "Word: one, Vector: [-0.22556999  0.49417999  0.48609999 -0.4332      0.13738     0.50616997\n",
      "  0.26058     0.30103001 -0.091486    0.10876     0.30579999  0.051028\n",
      "  0.22303     0.054236    0.068838   -0.24700999  0.32688999 -0.082203\n",
      " -0.28865999  0.3734      0.73803997 -0.040969    0.040201    0.11384\n",
      "  0.69986999 -0.49744999 -0.06755    -0.42598999 -0.10725    -0.010697\n",
      " -0.01479     0.55975997  0.3064      0.053053    0.058034    0.32756001\n",
      " -0.37233001  0.46513     0.14285    -0.085003   -0.45475999  0.19773\n",
      "  0.6383     -0.31147999  0.10858     0.31557     0.36682001 -0.35135001\n",
      " -0.48414001 -0.33234999 -0.33816001 -0.39678001  0.1908      1.3513\n",
      " -0.39043999 -2.87949991 -0.14275999 -0.087754    1.77129996  0.99331999\n",
      " -0.14128999  0.94388998  0.050897    0.47373     0.86387002 -0.16162001\n",
      "  0.67198998  0.52344     0.14438    -0.055194   -0.34669    -0.20742001\n",
      "  0.18907    -0.19845     0.34862     0.10121    -0.092119   -0.66258001\n",
      " -1.0582     -0.11803     0.70170999  0.077776   -0.50546002  0.032243\n",
      " -1.61759996 -0.29302001 -0.061748   -0.32473001  0.3439     -0.44698\n",
      "  0.085689    0.13294999 -0.1807     -0.11854    -0.82985002  0.13784\n",
      " -0.34358999 -0.45743999  0.49645999  0.34906   ]\n",
      "Word: like, Vector: [-0.2687      0.81708002  0.69896001 -0.72341001  0.091566    0.19557001\n",
      " -0.52112001 -0.24313    -0.44701001 -0.27039    -0.34125999 -0.46898001\n",
      "  0.42583001  0.46289     0.17106    -0.26795     0.23162     0.46568\n",
      " -0.31808001  0.75875002  0.31856999  0.64124     0.067042   -0.18516999\n",
      "  0.49996001  0.36963999 -0.31172001 -0.73097998 -0.26901999 -0.32058001\n",
      "  0.23394001  0.24276     0.1426     -0.2793      0.38823     0.42398\n",
      "  0.1021      0.33316001  0.30149999 -0.52710998 -0.024475   -0.15301\n",
      " -0.3224     -0.51231003 -0.55250001  0.29819     0.10847     0.052334\n",
      " -0.2298     -0.77889001 -0.08928     0.48109001  0.015368    0.92544001\n",
      " -0.26122001 -2.47589993 -0.019825    0.58280998  1.30599999  0.73512\n",
      " -0.34371999  1.58290005 -0.10814     0.11388     0.79220003  0.18347\n",
      "  1.22319996  0.35697001  0.17504001 -0.16527    -0.012827   -0.47918001\n",
      " -0.32111001 -0.40573001 -0.37151     0.086323    0.25172001 -0.082751\n",
      " -0.25584    -0.19178     1.0474     -0.51984    -0.71463001  0.38826999\n",
      " -1.67219996  0.015986   -0.22668    -0.26602    -0.57924998 -0.85650998\n",
      "  0.20543    -0.46371999 -0.065652   -0.061944   -0.57233    -0.46406001\n",
      " -0.41405001 -0.40110001  0.74656999  0.31121999]\n",
      "Word: could, Vector: [ 0.05869     0.40272999  0.38633999 -0.58888    -0.24626    -0.31042001\n",
      "  0.01324     0.075456    0.22054     0.064226   -0.28233001  0.69801003\n",
      "  0.39535001 -0.10333    -0.084886   -0.49865001  0.23266     0.59157002\n",
      " -0.75345999  0.075669    0.43555    -0.32304001  0.51099002  0.1227\n",
      "  0.010213   -0.17402001 -0.53118002 -0.59123999  0.21893001 -0.39508\n",
      "  0.10967     0.64652997 -0.32986     0.20874999  0.31209001  0.3969\n",
      "  0.23556     0.14959     0.044618   -0.039851   -0.67001998 -0.26655\n",
      " -0.22886001 -0.75891     0.023181    0.064913    0.63950002 -0.51099998\n",
      "  0.14996    -1.30280006  0.13912    -0.048588    0.011077    1.29610002\n",
      "  0.33386001 -2.5704999  -0.28143999 -0.20408     1.67939997  1.05340004\n",
      " -0.24247     1.06309998 -0.39963999  0.15578     1.11810005  0.19073001\n",
      "  0.44354999  0.59406    -0.34413001 -0.89059001  0.2529     -0.84498\n",
      " -0.021412   -0.53437001  0.16315    -0.35699999 -0.13122     0.057807\n",
      " -0.81308001  0.47248     0.91395003 -0.31516999 -0.80875999 -0.25029001\n",
      " -1.93910003 -0.66841     0.77363002  0.21205001 -0.17103    -0.44721001\n",
      " -0.25470999 -0.21399    -0.27142    -0.45612001 -0.40669     0.15552001\n",
      "  0.41519001 -0.35973999  0.43718001  0.10121   ]\n",
      "Word: time, Vector: [-2.42209993e-02 -3.48550007e-02  3.57100010e-01 -2.15499997e-02\n",
      " -5.48039973e-01  3.18219990e-01  1.29290000e-02  3.51740003e-01\n",
      " -4.16900009e-01 -2.94110000e-01  6.37229979e-01 -1.48589998e-01\n",
      " -1.52639998e-02 -3.58110011e-01  1.23949997e-01 -6.74679995e-01\n",
      "  2.81650007e-01 -6.54100021e-03 -4.17430013e-01  5.63869998e-03\n",
      "  2.67890006e-01  1.82820007e-01  7.70229995e-02  2.38079995e-01\n",
      "  5.04999995e-01 -2.49929994e-01 -2.91430000e-02 -5.06659985e-01\n",
      "  4.22149986e-01 -9.79880020e-02 -4.59969997e-01  3.79689991e-01\n",
      " -1.24020003e-01 -1.95680007e-01 -1.24789998e-01  2.40710005e-01\n",
      " -5.16560018e-01  4.48659986e-01 -4.04840000e-02 -4.10010010e-01\n",
      " -5.55760026e-01 -2.68920004e-01  1.93169996e-01  8.97540003e-02\n",
      " -2.79619992e-01 -1.56700000e-01  3.92760009e-01 -9.73730028e-01\n",
      "  4.41480011e-01 -9.75080013e-01  2.88680010e-03 -2.97740012e-01\n",
      "  3.33669990e-01  1.57980001e+00 -4.78329994e-02 -2.96989989e+00\n",
      " -4.16269988e-01  3.44710015e-02  1.69930005e+00  8.34699988e-01\n",
      " -2.89260000e-01  8.53070021e-01 -2.36080006e-01  1.26670003e-01\n",
      "  9.92020011e-01 -1.86010003e-01  2.80530006e-01  8.50469992e-02\n",
      "  2.76939988e-01 -3.00980005e-02 -3.05139989e-01 -2.96680003e-01\n",
      " -2.74710003e-02 -2.72199988e-01  1.37180001e-01 -1.25909999e-01\n",
      " -4.35090005e-01 -7.04609990e-01 -8.99529994e-01 -1.51150003e-01\n",
      "  5.04390001e-01 -2.39539996e-01 -5.21600008e-01  1.22239999e-01\n",
      " -1.46700001e+00  2.16159999e-01 -7.18249977e-02 -2.20669992e-02\n",
      " -1.51280001e-01 -5.50809979e-01 -2.85919994e-01  3.22690010e-01\n",
      "  1.98770002e-01  1.46630004e-01 -8.12129974e-01 -3.02949995e-01\n",
      "  1.98650002e-01 -8.75680000e-02  2.59609997e-01  5.07830009e-02]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a sequence:\", sequences[0])\n",
    "print(\"Example of a label:\", labels[0])  \n",
    "print(\"Embedding matrix:\", embedding_matrix)\n",
    "\n",
    "for word, i in list(word_index.items())[:5]:\n",
    "    print(f\"Word: {word}, Vector: {embedding_matrix[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1795f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 133s 2s/step - loss: 1.5454 - accuracy: 0.3218 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 1.5563 - val_accuracy: 0.3108 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 111s 1s/step - loss: 1.5218 - accuracy: 0.3289 - precision_4: 0.0000e+00 - recall_4: 0.0000e+00 - val_loss: 1.5529 - val_accuracy: 0.3050 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 116s 2s/step - loss: 1.5075 - accuracy: 0.3378 - precision_4: 0.7500 - recall_4: 6.2526e-04 - val_loss: 1.5555 - val_accuracy: 0.3108 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 120s 2s/step - loss: 1.4903 - accuracy: 0.3466 - precision_4: 0.5238 - recall_4: 0.0023 - val_loss: 1.5571 - val_accuracy: 0.3100 - val_precision_4: 0.0000e+00 - val_recall_4: 0.0000e+00\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 140s 2s/step - loss: 1.4779 - accuracy: 0.3501 - precision_4: 0.6316 - recall_4: 0.0025 - val_loss: 1.5581 - val_accuracy: 0.2967 - val_precision_4: 0.5000 - val_recall_4: 8.3333e-04\n",
      "38/38 [==============================] - 3s 62ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.02      0.04       173\n",
      "           1       0.29      0.34      0.32       292\n",
      "           2       0.24      0.15      0.19       216\n",
      "           3       0.50      0.01      0.01       142\n",
      "           4       0.31      0.58      0.40       377\n",
      "\n",
      "    accuracy                           0.30      1200\n",
      "   macro avg       0.34      0.22      0.19      1200\n",
      "weighted avg       0.32      0.30      0.24      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import classification_report\n",
    "import keras.backend as K\n",
    "\n",
    "# Model structure\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "inputs = Input(shape=(max_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(inputs)\n",
    "lstm = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)\n",
    "preds = Dense(labels.shape[1], activation='softmax')(lstm)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=preds)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64, callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test_argmax = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "print(classification_report(y_test_argmax, y_pred))\n",
    "\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f2170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d7338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
