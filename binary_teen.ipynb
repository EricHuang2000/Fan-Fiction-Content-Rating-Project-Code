{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1a3e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_bin = pd.read_csv(r'C:\\Users\\erich\\Desktop\\DS_project\\data\\data_30_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2aaff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               HTML_Content                                        TXT_Content\n",
      "0     Teen And Up Audiences  huckleberry finn stood front old wooden door m...\n",
      "1     Teen And Up Audiences  right huck trying figure every sort way could ...\n",
      "2                    Others  three buried mother christmas eve thing huck c...\n",
      "3     Teen And Up Audiences  air hot brow wet mind exhausted st petersburg ...\n",
      "4                    Others  episode genre al chaos chaos space time vortex...\n",
      "...                     ...                                                ...\n",
      "6036  Teen And Up Audiences  seen news pat asked robin chance hang coat mor...\n",
      "6037  Teen And Up Audiences  free next friday night murphy smiled phone rob...\n",
      "6038                 Others  turned walked away wan na say come baby give w...\n",
      "6039                 Others  perfect word tended apply liberally one often ...\n",
      "6040                 Others  heaven fact unlike earth pro con pro include t...\n",
      "\n",
      "[6005 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_bin_teen(df):\n",
    "    df = df[df['HTML_Content'].isin(['Explicit', 'General Audiences', 'Mature', 'Not Rated', 'Teen And Up Audiences'])]\n",
    "\n",
    "    df = df.dropna(subset=['TXT_Content'])\n",
    "\n",
    "    def clean_text(text):\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        text = text.lower()\n",
    "        tokenized_text = word_tokenize(text)\n",
    "        cleaned_text = [lemmatizer.lemmatize(word) for word in tokenized_text if word not in set(stopwords.words('english'))]\n",
    "\n",
    "        return ' '.join(cleaned_text)\n",
    "\n",
    "    df['TXT_Content'] = df['TXT_Content'].apply(clean_text)\n",
    "    \n",
    "    # 在'HTML_Content'列中，将除了'Teen And Up Audiences'以外的所有值改为'Others'\n",
    "    df['HTML_Content'] = df['HTML_Content'].apply(lambda x: 'Others' if x != 'Teen And Up Audiences' else x)\n",
    "\n",
    "    return df\n",
    "\n",
    "data_bin = clean_bin_teen(data_bin)\n",
    "print(data_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71fee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
